{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "\"\"\"Generate HDF5 file with all catchment model input data.\n",
    "\n",
    "Sources are DBF and some large text files.\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "\n",
    "import numpy\n",
    "import pandas\n",
    "import tables\n",
    "\n",
    "from crosswater.read_config import read_config\n",
    "from crosswater.tools import dbflib\n",
    "from crosswater.tools.hdf5_helpers import find_ids\n",
    "from crosswater.tools.time_helper import ProgressDisplay\n",
    "\n",
    "\n",
    "def read_dbf_cols(dbf_file_name, col_names=None):\n",
    "    \"\"\"Returns a dictionary with column names as keys and lists as values.\n",
    "    \n",
    "    Returns dict with all columns if `col_names` is false.\n",
    "    \n",
    "    dbf_file_name is a string\n",
    "    col_names is a list containing strings\n",
    "    \"\"\"\n",
    "    dbf_file = dbflib.DbfReader(dbf_file_name)\n",
    "    dbf_file.read_all()\n",
    "    if not col_names:\n",
    "        return dbf_file.data\n",
    "    res = {key: dbf_file.data[key] for key in col_names}\n",
    "    return res\n",
    "\n",
    "\n",
    "def read_dbf_col(dbf_file_name, col_name):\n",
    "    \"\"\"Retruns all column entries for a given column name.\n",
    "    \"\"\"\n",
    "    return read_dbf_cols(dbf_file_name, [col_name])[col_name]\n",
    "\n",
    "\n",
    "def get_value_by_id(dbf_file_name, col_name, converter=1, ids=None):\n",
    "    \"\"\"Returns a dict catchment-id: value\n",
    "    \n",
    "    converter for units with default value 1\n",
    "    ids to filter (e.g. only Strahler)\n",
    "    \"\"\"\n",
    "    data = read_dbf_cols(dbf_file_name, ['WSO1_ID', col_name])\n",
    "    res = {id_: value * converter for id_, value in\n",
    "           zip(data['WSO1_ID'], data[col_name])}\n",
    "    if ids:\n",
    "        res = {id_: value for id_, value in res.items() if id_ in ids}\n",
    "    return res\n",
    "\n",
    "\n",
    "def get_tot_areas(dbf_file_name, ids=None):\n",
    "    \"\"\"Returns a dict with catchment ids as keys and areas as values.\"\"\"\n",
    "    return get_value_by_id(dbf_file_name, 'AREA', ids=ids)\n",
    "\n",
    "\n",
    "def get_strahler(dbf_file_name, ids=None):\n",
    "    \"\"\"Returns a dict with catchment ids as keys and strahler as values.\"\"\"\n",
    "    return get_value_by_id(dbf_file_name, 'STRAHLER', ids=ids)\n",
    "\n",
    "\n",
    "def get_appl_areas(dbf_file_name, ids=None):\n",
    "    \"\"\"Returns a dict with catchment ids as keys and maiz areas as values.\"\"\"\n",
    "    return get_value_by_id(dbf_file_name, 'LMAIZ', converter=1e6, ids=ids)\n",
    "    \n",
    "\n",
    "def get_appl_rates(dbf_file_name, ids=None):\n",
    "    \"\"\"Returns a dict with catchments ids as keys and application rate as \n",
    "    values.\n",
    "    \"\"\"\n",
    "    return get_value_by_id(dbf_file_name, 'appl_rates', ids=ids)\n",
    "\n",
    "\n",
    "def filter_strahler_lessthan(strahler, tot_areas, appl_areas, appl_rates, strahler_limit=3):\n",
    "    \"\"\"Use only catchments where STRAHLER is <= limit.\n",
    "    \"\"\"\n",
    "\n",
    "    def apply_filter(old_values):\n",
    "        \"\"\"Filter for ids.\n",
    "        \"\"\"\n",
    "        return {id_: value for id_, value in old_values.items() if id_ in ids}\n",
    "    \n",
    "    ids = {id_ for id_, value in strahler.items() if value <= strahler_limit}\n",
    "    return (apply_filter(strahler), apply_filter(tot_areas),\n",
    "            apply_filter(appl_areas), apply_filter(appl_rates))\n",
    "\n",
    "\n",
    "class Parameters(tables.IsDescription):\n",
    "    # pylint: disable=too-few-public-methods\n",
    "    \"\"\"Table layout for parameters.\"\"\"\n",
    "    name = tables.StringCol(100)\n",
    "    value = tables.Float64Col()\n",
    "    unit = tables.StringCol(20)\n",
    "\n",
    "\n",
    "def create_hdf_file(file_name, tot_areas, appl_areas, appl_rates):\n",
    "    \"\"\"Create HDF5 file and add areas as parameters.\"\"\"\n",
    "    ids = sorted(tot_areas.keys())\n",
    "    h5_file = tables.open_file(file_name, mode='w',\n",
    "                               title='Input data for catchment models.')\n",
    "    for id_ in ids:\n",
    "        # create new group (where, name, title)        \n",
    "        group = h5_file.create_group('/', 'catch_{}'.format(id_),\n",
    "                                     'catchment {}'.format(id_))\n",
    "        # create new table (where, name, description, title)\n",
    "        table = h5_file.create_table(group, 'parameters', Parameters,\n",
    "                                     'constant parameters')\n",
    "        tot_area = tot_areas[id_]\n",
    "        appl_area = appl_areas[id_]\n",
    "        appl_rate = appl_rates[id_]\n",
    "        \n",
    "        # fill parameter table by rows\n",
    "        row = table.row\n",
    "        row['name'] = 'A_tot'\n",
    "        row['value'] = tot_area\n",
    "        row['unit'] = 'm**2'\n",
    "        row.append()\n",
    "        row['name'] = 'A_appl'\n",
    "        row['value'] = appl_area\n",
    "        row['unit'] = 'm**2'\n",
    "        row.append()\n",
    "        row['name'] = 'R_appl'\n",
    "        row['value'] = appl_rate\n",
    "        row['unit'] = 'g/m**2'\n",
    "        row.append()\n",
    "    h5_file.close()\n",
    "\n",
    "\n",
    "def add_input_tables(h5_file_name, t_file_name, p_file_name, q_file_name,\n",
    "                     batch_size=None, total=365 * 24):\n",
    "    \"\"\"Add input with pandas.\n",
    "    \"\"\"\n",
    "    # pylint: disable=too-many-locals\n",
    "    filters = tables.Filters(complevel=5, complib='zlib')\n",
    "    h5_file = tables.open_file(h5_file_name, mode='a')\n",
    "    get_child = h5_file.root._f_get_child  # pylint: disable=protected-access\n",
    "    all_ids = ids = find_ids(h5_file)\n",
    "    usecols = None\n",
    "    if batch_size is None:\n",
    "        batch_size = sys.maxsize\n",
    "    if batch_size < len(all_ids):\n",
    "        usecols = True\n",
    "    counter = 0\n",
    "    total_ids = len(all_ids)\n",
    "    prog = ProgressDisplay(total_ids)\n",
    "    # pylint: disable=undefined-loop-variable\n",
    "    while all_ids:\n",
    "        ids = all_ids[-batch_size:]\n",
    "        all_ids = all_ids[:-batch_size]\n",
    "        if usecols:\n",
    "            usecols = ids\n",
    "        temp = pandas.read_csv(t_file_name, sep=';', parse_dates=True,\n",
    "                               usecols=usecols)\n",
    "        precip = pandas.read_csv(p_file_name, sep=';', parse_dates=True,\n",
    "                                 usecols=usecols)\n",
    "        dis = pandas.read_csv(q_file_name, sep=';', parse_dates=True,\n",
    "                              usecols=usecols)\n",
    "        temp_hourly = temp.reindex(dis.index, method='ffill')\n",
    "        for id_ in ids:\n",
    "            counter += 1\n",
    "            inputs = pandas.concat([temp_hourly[id_], precip[id_], dis[id_]],\n",
    "                                   axis=1)\n",
    "            inputs.columns = ['temperature', 'precipitation', 'discharge']\n",
    "            input_table = inputs.to_records(index=False)\n",
    "            name = 'catch_{}'.format(id_)\n",
    "            group = get_child(name)\n",
    "            h5_file.create_table(group, 'inputs', input_table,\n",
    "                                 'time varying inputs', expectedrows=total,\n",
    "                                 filters=filters)\n",
    "            prog.show_progress(counter, additional=id_)\n",
    "    prog.show_progress(counter, additional=id_, force=True)\n",
    "    int_steps = pandas.DataFrame(dis.index.to_series()).astype(numpy.int64)\n",
    "    int_steps.columns = ['timesteps']\n",
    "    time_steps = int_steps.to_records(index=False)\n",
    "    h5_file.create_table('/', 'time_steps', time_steps,\n",
    "                         'time steps for all catchments')\n",
    "    h5_file.close()\n",
    "\n",
    "\n",
    "def get_first_ids(q_file_name, max_ids):\n",
    "    \"\"\"Get first `max_ids` from the dicharge file.\n",
    "    \"\"\"\n",
    "    with open(q_file_name) as fobj:\n",
    "        header = next(fobj).strip().split(';')\n",
    "    return {int(entry[1:-1]) for entry in header[:max_ids]}\n",
    "\n",
    "\n",
    "def preprocess(config_file):\n",
    "    \"\"\"Do the preprocessing.\n",
    "    \"\"\"\n",
    "    config = read_config(config_file)\n",
    "    h5_file_name = config['preprocessing']['hdf_input_path']\n",
    "    t_file_name = config['preprocessing']['temperature_path']\n",
    "    p_file_name = config['preprocessing']['precipitation_path']\n",
    "    q_file_name = config['preprocessing']['discharge_path']\n",
    "    max_ids = config['preprocessing']['max_ids']\n",
    "    ids = None\n",
    "    if max_ids:\n",
    "        ids = get_first_ids(q_file_name, max_ids)\n",
    "    batch_size = config['preprocessing']['batch_size']\n",
    "    strahler = get_strahler(config['preprocessing']['catchment_path'], ids)\n",
    "    tot_areas = get_tot_areas(config['preprocessing']['catchment_path'], ids)\n",
    "    \n",
    "    crops = config['preprocessing']['crops'].split(', ')    \n",
    "    appl_areas = get_appl_areas(config['preprocessing']['landuse_path'], ids)\n",
    "\n",
    "    appl_rates = get_appl_rates(config['preprocessing']['micropollutant_path'], ids)\n",
    "    strahler_limit = config['preprocessing']['strahler_limit']\n",
    "    strahler, tot_areas, appl_areas, appl_rates = filter_strahler_lessthan(\n",
    "        strahler, tot_areas, appl_areas, appl_rates, strahler_limit)\n",
    "    create_hdf_file(h5_file_name, tot_areas, appl_areas, appl_rates)\n",
    "    add_input_tables(h5_file_name, t_file_name, p_file_name, q_file_name,\n",
    "                     batch_size=batch_size)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "config = read_config(\"config.ini\")\n",
    "h5_file_name = config['preprocessing']['hdf_input_path']\n",
    "t_file_name = config['preprocessing']['temperature_path']\n",
    "p_file_name = config['preprocessing']['precipitation_path']\n",
    "q_file_name = config['preprocessing']['discharge_path']\n",
    "max_ids = config['preprocessing']['max_ids']\n",
    "ids = None\n",
    "if max_ids:\n",
    "    ids = get_first_ids(q_file_name, max_ids)\n",
    "batch_size = config['preprocessing']['batch_size']\n",
    "strahler = get_strahler(config['preprocessing']['catchment_path'], ids)\n",
    "tot_areas = get_tot_areas(config['preprocessing']['catchment_path'], ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "crops = config['preprocessing']['crops'].split(', ')    \n",
    "appl_areas = get_appl_areas(config['preprocessing']['landuse_path'], ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LMAIZ', 'BARL', 'SWHE']"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{346350, 346351, 346352, 346358, 346359}"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dbf_file_name=config['preprocessing']['landuse_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.test>"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_appl_areas(dbf_file_name, crops, ids=None):\n",
    "    \"\"\"Returns a dict with catchment ids as keys and maiz areas as values.\"\"\"\n",
    "    res = get_value_by_id(dbf_file_name, crop, converter=1e6, ids=ids)\n",
    "    for crop in crops:\n",
    "        crop=get_value_by_id(dbf_file_name, crop, converter=1e6, ids=ids)\n",
    "\n",
    "    return test\n",
    "\n",
    "get_appl_areas(config['preprocessing']['landuse_path'], crops, ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_appl_areas(dbf_file_name, ids=None):\n",
    "    \"\"\"Returns a dict with catchment ids as keys and maiz areas as values.\"\"\"\n",
    "    return get_value_by_id(dbf_file_name, 'LMAIZ', converter=1e6, ids=ids)\n",
    "res1=get_appl_areas(dbf_file_name, ids)\n",
    "\n",
    "def get_appl_areas(dbf_file_name, ids=None):\n",
    "    \"\"\"Returns a dict with catchment ids as keys and maiz areas as values.\"\"\"\n",
    "    return get_value_by_id(dbf_file_name, 'BARL', converter=1e6, ids=ids)\n",
    "res2=get_appl_areas(dbf_file_name, ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{346350.0: 1655200.0,\n",
       " 346351.0: 1044200.0,\n",
       " 346352.0: 0.0,\n",
       " 346358.0: 192800.0,\n",
       " 346359.0: 1088700.0}"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{346350.0: 286500.0,\n",
       " 346351.0: 404800.0,\n",
       " 346352.0: 0.0,\n",
       " 346358.0: 397800.0,\n",
       " 346359.0: 117200.0}"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function dict.values>"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res1.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "A = zip(res1.values(), res2.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t=zip(res1.keys(), zip(res1.values(), res2.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(346352.0, (0.0, 0.0))"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "590600.0"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(next(t)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{346350.0: 1941700.0,\n",
       " 346351.0: 1449000.0,\n",
       " 346352.0: 0.0,\n",
       " 346358.0: 590600.0,\n",
       " 346359.0: 1205900.0}"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{id_: sum(values) for id_, values in zip(res1.keys(), zip(res1.values(), res2.values()))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "crops = ['LMAIZ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_appl_areas(dbf_file_name, crops, ids=None):\n",
    "    \"\"\"Returns a dict with catchment ids as keys and maiz areas as values.\"\"\"\n",
    "    \n",
    "    res = get_value_by_id(dbf_file_name, crops[0], converter=1e6, ids=ids)\n",
    "    \n",
    "    for crop in crops[1:]:\n",
    "        res_crop = get_value_by_id(dbf_file_name, crop, converter=1e6, ids=ids)\n",
    "        res = {id_: sum(values) for id_, values in zip(res.keys(), zip(res.values(), res_crop.values()))}\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res = get_value_by_id(dbf_file_name, crops[0], converter=1e6, ids=ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{346350.0: 1655200.0,\n",
       " 346351.0: 1044200.0,\n",
       " 346352.0: 0.0,\n",
       " 346358.0: 192800.0,\n",
       " 346359.0: 1088700.0}"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for crop in crops[1:]:\n",
    "    res_crop = get_value_by_id(dbf_file_name, crop, converter=1e6, ids=ids)\n",
    "    res = {id_: sum(values) for id_, values in zip(res.keys(), zip(res.values(), res_crop.values()))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{346350.0: 1655200.0,\n",
       " 346351.0: 1044200.0,\n",
       " 346352.0: 0.0,\n",
       " 346358.0: 192800.0,\n",
       " 346359.0: 1088700.0}"
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = get_value_by_id(dbf_file_name, crops[0], converter=1e6, ids=ids)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "crop=crops[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{346350.0: 286500.0,\n",
       " 346351.0: 404800.0,\n",
       " 346352.0: 0.0,\n",
       " 346358.0: 397800.0,\n",
       " 346359.0: 117200.0}"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_crop = get_value_by_id(dbf_file_name, crop, converter=1e6, ids=ids)\n",
    "res_crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{346350.0: 1941700.0,\n",
       " 346351.0: 1449000.0,\n",
       " 346352.0: 0.0,\n",
       " 346358.0: 590600.0,\n",
       " 346359.0: 1205900.0}"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = {id_: sum(values) for id_, values in zip(res.keys(), zip(res.values(), res_crop.values()))}\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{346350.0: 1632400.0,\n",
       " 346351.0: 1152500.0,\n",
       " 346352.0: 0.0,\n",
       " 346358.0: 570400.0,\n",
       " 346359.0: 1157500.0}"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crop=crops[2]\n",
    "res_crop = get_value_by_id(dbf_file_name, crop, converter=1e6, ids=ids)\n",
    "res_crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{346350.0: 3287600.0,\n",
       " 346351.0: 2196700.0,\n",
       " 346352.0: 0.0,\n",
       " 346358.0: 763200.0,\n",
       " 346359.0: 2246200.0}"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = {id_: sum(values) for id_, values in zip(res.keys(), zip(res.values(), res_crop.values()))}\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test=zip(res.keys(), zip(res.values(), res_crop.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([0.0, 1157500.0, 570400.0, 1632400.0, 1152500.0])"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_crop.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([0.0, 2196700.0, 3287600.0, 763200.0, 2246200.0])"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    ">>> d1 = {'a': 1, 'b': 2} \n",
    ">>> d2 = {'b': 1, 'c': 3}\n",
    ">>> d2.update(d1)\n",
    ">>> d2\n",
    "{'a': 1, 'c': 3, 'b': 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d3 = dict(d1, **d2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
