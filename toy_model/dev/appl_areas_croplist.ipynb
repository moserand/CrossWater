{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "\"\"\"Generate HDF5 file with all catchment model input data.\n",
    "\n",
    "Sources are DBF and some large text files.\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "\n",
    "import numpy\n",
    "import pandas\n",
    "import tables\n",
    "\n",
    "from crosswater.read_config import read_config\n",
    "from crosswater.tools import dbflib\n",
    "from crosswater.tools.hdf5_helpers import find_ids\n",
    "from crosswater.tools.time_helper import ProgressDisplay\n",
    "\n",
    "\n",
    "def read_dbf_cols(dbf_file_name, col_names=None):\n",
    "    \"\"\"Returns a dictionary with column names as keys and lists as values.\n",
    "    \n",
    "    Returns dict with all columns if `col_names` is false.\n",
    "    \n",
    "    dbf_file_name is a string\n",
    "    col_names is a list containing strings\n",
    "    \"\"\"\n",
    "    dbf_file = dbflib.DbfReader(dbf_file_name)\n",
    "    dbf_file.read_all()\n",
    "    if not col_names:\n",
    "        return dbf_file.data\n",
    "    res = {key: dbf_file.data[key] for key in col_names}\n",
    "    return res\n",
    "\n",
    "\n",
    "def read_dbf_col(dbf_file_name, col_name):\n",
    "    \"\"\"Retruns all column entries for a given column name.\n",
    "    \"\"\"\n",
    "    return read_dbf_cols(dbf_file_name, [col_name])[col_name]\n",
    "\n",
    "\n",
    "def get_value_by_id(dbf_file_name, col_name, converter=1, ids=None):\n",
    "    \"\"\"Returns a dict catchment-id: value\n",
    "    \n",
    "    converter for units with default value 1\n",
    "    ids to filter (e.g. only Strahler)\n",
    "    \"\"\"\n",
    "    data = read_dbf_cols(dbf_file_name, ['WSO1_ID', col_name])\n",
    "    res = {id_: value * converter for id_, value in\n",
    "           zip(data['WSO1_ID'], data[col_name])}\n",
    "    if ids:\n",
    "        res = {id_: value for id_, value in res.items() if id_ in ids}\n",
    "    return res\n",
    "\n",
    "\n",
    "def get_tot_areas(dbf_file_name, ids=None):\n",
    "    \"\"\"Returns a dict with catchment ids as keys and areas as values.\"\"\"\n",
    "    return get_value_by_id(dbf_file_name, 'AREA', ids=ids)\n",
    "\n",
    "\n",
    "def get_strahler(dbf_file_name, ids=None):\n",
    "    \"\"\"Returns a dict with catchment ids as keys and strahler as values.\"\"\"\n",
    "    return get_value_by_id(dbf_file_name, 'STRAHLER', ids=ids)\n",
    "\n",
    "\n",
    "def get_appl_areas(dbf_file_name, ids=None):\n",
    "    \"\"\"Returns a dict with catchment ids as keys and maiz areas as values.\"\"\"\n",
    "    return get_value_by_id(dbf_file_name, 'LMAIZ', converter=1e6, ids=ids)\n",
    "    \n",
    "\n",
    "def get_appl_rates(dbf_file_name, ids=None):\n",
    "    \"\"\"Returns a dict with catchments ids as keys and application rate as \n",
    "    values.\n",
    "    \"\"\"\n",
    "    return get_value_by_id(dbf_file_name, 'appl_rates', ids=ids)\n",
    "\n",
    "\n",
    "def filter_strahler_lessthan(strahler, tot_areas, appl_areas, appl_rates, strahler_limit=3):\n",
    "    \"\"\"Use only catchments where STRAHLER is <= limit.\n",
    "    \"\"\"\n",
    "\n",
    "    def apply_filter(old_values):\n",
    "        \"\"\"Filter for ids.\n",
    "        \"\"\"\n",
    "        return {id_: value for id_, value in old_values.items() if id_ in ids}\n",
    "    \n",
    "    ids = {id_ for id_, value in strahler.items() if value <= strahler_limit}\n",
    "    return (apply_filter(strahler), apply_filter(tot_areas),\n",
    "            apply_filter(appl_areas), apply_filter(appl_rates))\n",
    "\n",
    "\n",
    "class Parameters(tables.IsDescription):\n",
    "    # pylint: disable=too-few-public-methods\n",
    "    \"\"\"Table layout for parameters.\"\"\"\n",
    "    name = tables.StringCol(100)\n",
    "    value = tables.Float64Col()\n",
    "    unit = tables.StringCol(20)\n",
    "\n",
    "\n",
    "def create_hdf_file(file_name, tot_areas, appl_areas, appl_rates):\n",
    "    \"\"\"Create HDF5 file and add areas as parameters.\"\"\"\n",
    "    ids = sorted(tot_areas.keys())\n",
    "    h5_file = tables.open_file(file_name, mode='w',\n",
    "                               title='Input data for catchment models.')\n",
    "    for id_ in ids:\n",
    "        # create new group (where, name, title)        \n",
    "        group = h5_file.create_group('/', 'catch_{}'.format(id_),\n",
    "                                     'catchment {}'.format(id_))\n",
    "        # create new table (where, name, description, title)\n",
    "        table = h5_file.create_table(group, 'parameters', Parameters,\n",
    "                                     'constant parameters')\n",
    "        tot_area = tot_areas[id_]\n",
    "        appl_area = appl_areas[id_]\n",
    "        appl_rate = appl_rates[id_]\n",
    "        \n",
    "        # fill parameter table by rows\n",
    "        row = table.row\n",
    "        row['name'] = 'A_tot'\n",
    "        row['value'] = tot_area\n",
    "        row['unit'] = 'm**2'\n",
    "        row.append()\n",
    "        row['name'] = 'A_appl'\n",
    "        row['value'] = appl_area\n",
    "        row['unit'] = 'm**2'\n",
    "        row.append()\n",
    "        row['name'] = 'R_appl'\n",
    "        row['value'] = appl_rate\n",
    "        row['unit'] = 'g/m**2'\n",
    "        row.append()\n",
    "    h5_file.close()\n",
    "\n",
    "\n",
    "def add_input_tables(h5_file_name, t_file_name, p_file_name, q_file_name,\n",
    "                     batch_size=None, total=365 * 24):\n",
    "    \"\"\"Add input with pandas.\n",
    "    \"\"\"\n",
    "    # pylint: disable=too-many-locals\n",
    "    filters = tables.Filters(complevel=5, complib='zlib')\n",
    "    h5_file = tables.open_file(h5_file_name, mode='a')\n",
    "    get_child = h5_file.root._f_get_child  # pylint: disable=protected-access\n",
    "    all_ids = ids = find_ids(h5_file)\n",
    "    usecols = None\n",
    "    if batch_size is None:\n",
    "        batch_size = sys.maxsize\n",
    "    if batch_size < len(all_ids):\n",
    "        usecols = True\n",
    "    counter = 0\n",
    "    total_ids = len(all_ids)\n",
    "    prog = ProgressDisplay(total_ids)\n",
    "    # pylint: disable=undefined-loop-variable\n",
    "    while all_ids:\n",
    "        ids = all_ids[-batch_size:]\n",
    "        all_ids = all_ids[:-batch_size]\n",
    "        if usecols:\n",
    "            usecols = ids\n",
    "        temp = pandas.read_csv(t_file_name, sep=';', parse_dates=True,\n",
    "                               usecols=usecols)\n",
    "        precip = pandas.read_csv(p_file_name, sep=';', parse_dates=True,\n",
    "                                 usecols=usecols)\n",
    "        dis = pandas.read_csv(q_file_name, sep=';', parse_dates=True,\n",
    "                              usecols=usecols)\n",
    "        temp_hourly = temp.reindex(dis.index, method='ffill')\n",
    "        for id_ in ids:\n",
    "            counter += 1\n",
    "            inputs = pandas.concat([temp_hourly[id_], precip[id_], dis[id_]],\n",
    "                                   axis=1)\n",
    "            inputs.columns = ['temperature', 'precipitation', 'discharge']\n",
    "            input_table = inputs.to_records(index=False)\n",
    "            name = 'catch_{}'.format(id_)\n",
    "            group = get_child(name)\n",
    "            h5_file.create_table(group, 'inputs', input_table,\n",
    "                                 'time varying inputs', expectedrows=total,\n",
    "                                 filters=filters)\n",
    "            prog.show_progress(counter, additional=id_)\n",
    "    prog.show_progress(counter, additional=id_, force=True)\n",
    "    int_steps = pandas.DataFrame(dis.index.to_series()).astype(numpy.int64)\n",
    "    int_steps.columns = ['timesteps']\n",
    "    time_steps = int_steps.to_records(index=False)\n",
    "    h5_file.create_table('/', 'time_steps', time_steps,\n",
    "                         'time steps for all catchments')\n",
    "    h5_file.close()\n",
    "\n",
    "\n",
    "def get_first_ids(q_file_name, max_ids):\n",
    "    \"\"\"Get first `max_ids` from the dicharge file.\n",
    "    \"\"\"\n",
    "    with open(q_file_name) as fobj:\n",
    "        header = next(fobj).strip().split(';')\n",
    "    return {int(entry[1:-1]) for entry in header[:max_ids]}\n",
    "\n",
    "\n",
    "def preprocess(config_file):\n",
    "    \"\"\"Do the preprocessing.\n",
    "    \"\"\"\n",
    "    config = read_config(config_file)\n",
    "    h5_file_name = config['preprocessing']['hdf_input_path']\n",
    "    t_file_name = config['preprocessing']['temperature_path']\n",
    "    p_file_name = config['preprocessing']['precipitation_path']\n",
    "    q_file_name = config['preprocessing']['discharge_path']\n",
    "    max_ids = config['preprocessing']['max_ids']\n",
    "    ids = None\n",
    "    if max_ids:\n",
    "        ids = get_first_ids(q_file_name, max_ids)\n",
    "    batch_size = config['preprocessing']['batch_size']\n",
    "    strahler = get_strahler(config['preprocessing']['catchment_path'], ids)\n",
    "    tot_areas = get_tot_areas(config['preprocessing']['catchment_path'], ids)\n",
    "    \n",
    "    crops = config['preprocessing']['crops'].split(', ')    \n",
    "    appl_areas = get_appl_areas(config['preprocessing']['landuse_path'], ids)\n",
    "\n",
    "    appl_rates = get_appl_rates(config['preprocessing']['micropollutant_path'], ids)\n",
    "    strahler_limit = config['preprocessing']['strahler_limit']\n",
    "    strahler, tot_areas, appl_areas, appl_rates = filter_strahler_lessthan(\n",
    "        strahler, tot_areas, appl_areas, appl_rates, strahler_limit)\n",
    "    create_hdf_file(h5_file_name, tot_areas, appl_areas, appl_rates)\n",
    "    add_input_tables(h5_file_name, t_file_name, p_file_name, q_file_name,\n",
    "                     batch_size=batch_size)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "config = read_config(\"config.ini\")\n",
    "h5_file_name = config['preprocessing']['hdf_input_path']\n",
    "t_file_name = config['preprocessing']['temperature_path']\n",
    "p_file_name = config['preprocessing']['precipitation_path']\n",
    "q_file_name = config['preprocessing']['discharge_path']\n",
    "max_ids = config['preprocessing']['max_ids']\n",
    "ids = None\n",
    "if max_ids:\n",
    "    ids = get_first_ids(q_file_name, max_ids)\n",
    "batch_size = config['preprocessing']['batch_size']\n",
    "strahler = get_strahler(config['preprocessing']['catchment_path'], ids)\n",
    "tot_areas = get_tot_areas(config['preprocessing']['catchment_path'], ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "crops = config['preprocessing']['crops'].split(', ')    \n",
    "appl_areas = get_appl_areas(config['preprocessing']['landuse_path'], ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LMAIZ', 'BARL', 'SWHE']"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{346350, 346351, 346352, 346358, 346359}"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dbf_file_name=config['preprocessing']['landuse_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.test>"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_appl_areas(dbf_file_name, crops, ids=None):\n",
    "    \"\"\"Returns a dict with catchment ids as keys and maiz areas as values.\"\"\"\n",
    "    res = get_value_by_id(dbf_file_name, crop, converter=1e6, ids=ids)\n",
    "    for crop in crops:\n",
    "        crop=get_value_by_id(dbf_file_name, crop, converter=1e6, ids=ids)\n",
    "\n",
    "    return test\n",
    "\n",
    "get_appl_areas(config['preprocessing']['landuse_path'], crops, ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_appl_areas(dbf_file_name, ids=None):\n",
    "    \"\"\"Returns a dict with catchment ids as keys and maiz areas as values.\"\"\"\n",
    "    return get_value_by_id(dbf_file_name, 'LMAIZ', converter=1e6, ids=ids)\n",
    "res1=get_appl_areas(dbf_file_name, ids)\n",
    "\n",
    "def get_appl_areas(dbf_file_name, ids=None):\n",
    "    \"\"\"Returns a dict with catchment ids as keys and maiz areas as values.\"\"\"\n",
    "    return get_value_by_id(dbf_file_name, 'BARL', converter=1e6, ids=ids)\n",
    "res2=get_appl_areas(dbf_file_name, ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{346350.0: 1655200.0,\n",
       " 346351.0: 1044200.0,\n",
       " 346352.0: 0.0,\n",
       " 346358.0: 192800.0,\n",
       " 346359.0: 1088700.0}"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{346350.0: 286500.0,\n",
       " 346351.0: 404800.0,\n",
       " 346352.0: 0.0,\n",
       " 346358.0: 397800.0,\n",
       " 346359.0: 117200.0}"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function dict.values>"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res1.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "A = zip(res1.values(), res2.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t=zip(res1.keys(), zip(res1.values(), res2.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(346352.0, (0.0, 0.0))"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "590600.0"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(next(t)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{346350.0: 1941700.0,\n",
       " 346351.0: 1449000.0,\n",
       " 346352.0: 0.0,\n",
       " 346358.0: 590600.0,\n",
       " 346359.0: 1205900.0}"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{id_: sum(values) for id_, values in zip(res1.keys(), zip(res1.values(), res2.values()))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "crops = ['LMAIZ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LMAIZ\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{346350.0: 1655200.0,\n",
       " 346351.0: 1044200.0,\n",
       " 346352.0: 0.0,\n",
       " 346358.0: 192800.0,\n",
       " 346359.0: 1088700.0}"
      ]
     },
     "execution_count": 443,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_appl_areas(dbf_file_name, crops, ids=None):\n",
    "    \"\"\"Returns a dict with catchment ids as keys and maiz areas as values.\"\"\"\n",
    "    \n",
    "    res = get_value_by_id(dbf_file_name, crops[0], converter=1e6, ids=ids)\n",
    "    print(crops[0])\n",
    "    \n",
    "    for crop in crops[1:]:\n",
    "        res_crop = get_value_by_id(dbf_file_name, crop, converter=1e6, ids=ids)\n",
    "        res = {id_: res.get(id_, 0) + res_crop.get(id_, 0) for id_ in set(res) & set(res_crop) }\n",
    "        print(crop)\n",
    "    \n",
    "    return res\n",
    "res = get_appl_areas(dbf_file_name, crops, ids)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{346350.0: 1655200.0,\n",
       " 346351.0: 1044200.0,\n",
       " 346352.0: 0.0,\n",
       " 346358.0: 192800.0,\n",
       " 346359.0: 1088700.0}"
      ]
     },
     "execution_count": 436,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res0 = get_value_by_id(dbf_file_name, crops[0], converter=1e6, ids=ids)\n",
    "res0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{346350.0: 286500.0,\n",
       " 346351.0: 404800.0,\n",
       " 346352.0: 0.0,\n",
       " 346358.0: 397800.0,\n",
       " 346359.0: 117200.0}"
      ]
     },
     "execution_count": 433,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crop=crops[1]\n",
    "res1 = get_value_by_id(dbf_file_name, crop, converter=1e6, ids=ids)\n",
    "res1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{346350.0: 1632400.0,\n",
       " 346351.0: 1152500.0,\n",
       " 346352.0: 0.0,\n",
       " 346358.0: 570400.0,\n",
       " 346359.0: 1157500.0}"
      ]
     },
     "execution_count": 434,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crop=crops[2]\n",
    "res2 = get_value_by_id(dbf_file_name, crop, converter=1e6, ids=ids)\n",
    "res2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LMAIZ'"
      ]
     },
     "execution_count": 444,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crops[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{346350.0: 1655200.0,\n",
       " 346351.0: 1044200.0,\n",
       " 346352.0: 0.0,\n",
       " 346358.0: 192800.0,\n",
       " 346359.0: 1088700.0}"
      ]
     },
     "execution_count": 449,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_value_by_id(dbf_file_name, crops[0], converter=1e6, ids=ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "crops = config['preprocessing']['crops'].split(', ') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LMAIZ'"
      ]
     },
     "execution_count": 448,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crops[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
