{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "\"\"\"Generate HDF5 file with all catchment model input data.\n",
    "\n",
    "Sources are DBF and some large text files.\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "\n",
    "import numpy\n",
    "import pandas\n",
    "import tables\n",
    "\n",
    "from crosswater.read_config import read_config\n",
    "from crosswater.tools import dbflib\n",
    "from crosswater.tools.hdf5_helpers import find_ids\n",
    "from crosswater.tools.time_helper import ProgressDisplay\n",
    "\n",
    "\n",
    "def read_dbf_cols(dbf_file_name, col_names=None):\n",
    "    \"\"\"Returns a dictionary with column names as keys and lists as values.\n",
    "    \n",
    "    Returns dict with all columns if `col_names` is false.\n",
    "    \n",
    "    dbf_file_name is a string\n",
    "    col_names is a list containing strings\n",
    "    \"\"\"\n",
    "    dbf_file = dbflib.DbfReader(dbf_file_name)\n",
    "    dbf_file.read_all()\n",
    "    if not col_names:\n",
    "        return dbf_file.data\n",
    "    res = {key: dbf_file.data[key] for key in col_names}\n",
    "    return res\n",
    "\n",
    "\n",
    "def read_dbf_col(dbf_file_name, col_name):\n",
    "    \"\"\"Retruns all column entries for a given column name.\n",
    "    \"\"\"\n",
    "    return read_dbf_cols(dbf_file_name, [col_name])[col_name]\n",
    "\n",
    "\n",
    "def get_value_by_id(dbf_file_name, col_name, converter=1, ids=None):\n",
    "    \"\"\"Returns a dict catchment-id: value\n",
    "    \n",
    "    converter for units with default value 1\n",
    "    ids to filter (e.g. only Strahler)\n",
    "    \"\"\"\n",
    "    data = read_dbf_cols(dbf_file_name, ['WSO1_ID', col_name])\n",
    "    res = {id_: value * converter for id_, value in\n",
    "           zip(data['WSO1_ID'], data[col_name])}\n",
    "    if ids:\n",
    "        res = {id_: value for id_, value in res.items() if id_ in ids}\n",
    "    return res\n",
    "\n",
    "\n",
    "def get_tot_areas(dbf_file_name, ids=None):\n",
    "    \"\"\"Returns a dict with catchment ids as keys and areas as values.\"\"\"\n",
    "    return get_value_by_id(dbf_file_name, 'AREA', ids=ids)\n",
    "\n",
    "\n",
    "def get_strahler(dbf_file_name, ids=None):\n",
    "    \"\"\"Returns a dict with catchment ids as keys and strahler as values.\"\"\"\n",
    "    return get_value_by_id(dbf_file_name, 'STRAHLER', ids=ids)\n",
    "\n",
    "\n",
    "def get_appl_areas(dbf_file_name, ids=None):\n",
    "    \"\"\"Returns a dict with catchment ids as keys and maiz areas as values.\"\"\"\n",
    "    return get_value_by_id(dbf_file_name, 'LMAIZ', converter=1e6, ids=ids)\n",
    "#def get_appl_areas(dbf_file_name, crops, ids=None):\n",
    "#    \"\"\"Returns a dict with catchment ids as keys and total areas as values.\n",
    "#\n",
    "#    Crop areas are added to total area.    \n",
    "#    \"\"\"\n",
    "#    res = get_value_by_id(dbf_file_name, crops[0], converter=1e6, ids=ids)\n",
    "#    for crop in crops[1:]:\n",
    "#        res_crop = get_value_by_id(dbf_file_name, crop, converter=1e6, ids=ids)\n",
    "#        res = {id_: res.get(id_, 0) + res_crop.get(id_, 0) for id_ in set(res) & set(res_crop) }\n",
    "#    return res\n",
    "    \n",
    "\n",
    "def get_appl_rates(dbf_file_name, ids=None):\n",
    "    \"\"\"Returns a dict with catchments ids as keys and application rate as \n",
    "    values.\n",
    "    \"\"\"\n",
    "    return get_value_by_id(dbf_file_name, 'appl_rates', ids=ids)\n",
    "\n",
    "\n",
    "def filter_strahler_lessthan(strahler, tot_areas, appl_areas, appl_rates, strahler_limit=3):\n",
    "    \"\"\"Use only catchments where STRAHLER is <= limit.\n",
    "    \"\"\"\n",
    "\n",
    "    def apply_filter(old_values):\n",
    "        \"\"\"Filter for ids.\n",
    "        \"\"\"\n",
    "        return {id_: value for id_, value in old_values.items() if id_ in ids}\n",
    "    \n",
    "    ids = {id_ for id_, value in strahler.items() if value <= strahler_limit}\n",
    "    return (apply_filter(strahler), apply_filter(tot_areas),\n",
    "            apply_filter(appl_areas), apply_filter(appl_rates))\n",
    "\n",
    "\n",
    "class Parameters(tables.IsDescription):\n",
    "    # pylint: disable=too-few-public-methods\n",
    "    \"\"\"Table layout for parameters.\"\"\"\n",
    "    name = tables.StringCol(100)\n",
    "    value = tables.Float64Col()\n",
    "    unit = tables.StringCol(20)\n",
    "\n",
    "\n",
    "def create_hdf_file(file_name, tot_areas, appl_areas, appl_rates):\n",
    "    \"\"\"Create HDF5 file and add areas as parameters.\"\"\"\n",
    "    ids = sorted(tot_areas.keys())\n",
    "    h5_file = tables.open_file(file_name, mode='w',\n",
    "                               title='Input data for catchment models.')\n",
    "    for id_ in ids:\n",
    "        # create new group (where, name, title)        \n",
    "        group = h5_file.create_group('/', 'catch_{}'.format(id_),\n",
    "                                     'catchment {}'.format(id_))\n",
    "        # create new table (where, name, description, title)\n",
    "        table = h5_file.create_table(group, 'parameters', Parameters,\n",
    "                                     'constant parameters')\n",
    "        tot_area = tot_areas[id_]\n",
    "        appl_area = appl_areas[id_]\n",
    "        appl_rate = appl_rates[id_]\n",
    "        \n",
    "        # fill parameter table by rows\n",
    "        row = table.row\n",
    "        row['name'] = 'A_tot'\n",
    "        row['value'] = tot_area\n",
    "        row['unit'] = 'm**2'\n",
    "        row.append()\n",
    "        row['name'] = 'A_appl'\n",
    "        row['value'] = appl_area\n",
    "        row['unit'] = 'm**2'\n",
    "        row.append()\n",
    "        row['name'] = 'R_appl'\n",
    "        row['value'] = appl_rate\n",
    "        row['unit'] = 'g/m**2'\n",
    "        row.append()\n",
    "    h5_file.close()\n",
    "\n",
    "\n",
    "def add_input_tables(h5_file_name, t_file_name, p_file_name, q_file_name,\n",
    "                     batch_size=None, total=365 * 24):\n",
    "    \"\"\"Add input with pandas.\n",
    "    \"\"\"\n",
    "    # pylint: disable=too-many-locals\n",
    "    filters = tables.Filters(complevel=5, complib='zlib')\n",
    "    h5_file = tables.open_file(h5_file_name, mode='a')\n",
    "    get_child = h5_file.root._f_get_child  # pylint: disable=protected-access\n",
    "    all_ids = ids = find_ids(h5_file)\n",
    "    usecols = None\n",
    "    if batch_size is None:\n",
    "        batch_size = sys.maxsize\n",
    "    if batch_size < len(all_ids):\n",
    "        usecols = True\n",
    "    counter = 0\n",
    "    total_ids = len(all_ids)\n",
    "    prog = ProgressDisplay(total_ids)\n",
    "    # pylint: disable=undefined-loop-variable\n",
    "    while all_ids:\n",
    "        ids = all_ids[-batch_size:]\n",
    "        all_ids = all_ids[:-batch_size]\n",
    "        if usecols:\n",
    "            usecols = ids\n",
    "        temp = pandas.read_csv(t_file_name, sep=';', parse_dates=True,\n",
    "                               usecols=usecols)\n",
    "        precip = pandas.read_csv(p_file_name, sep=';', parse_dates=True,\n",
    "                                 usecols=usecols)\n",
    "        dis = pandas.read_csv(q_file_name, sep=';', parse_dates=True,\n",
    "                              usecols=usecols)\n",
    "        temp_hourly = temp.reindex(dis.index, method='ffill')\n",
    "        for id_ in ids:\n",
    "            counter += 1\n",
    "            inputs = pandas.concat([temp_hourly[id_], precip[id_], dis[id_]],\n",
    "                                   axis=1)\n",
    "            inputs.columns = ['temperature', 'precipitation', 'discharge']\n",
    "            input_table = inputs.to_records(index=False)\n",
    "            name = 'catch_{}'.format(id_)\n",
    "            group = get_child(name)\n",
    "            h5_file.create_table(group, 'inputs', input_table,\n",
    "                                 'time varying inputs', expectedrows=total,\n",
    "                                 filters=filters)\n",
    "            prog.show_progress(counter, additional=id_)\n",
    "    prog.show_progress(counter, additional=id_, force=True)\n",
    "    int_steps = pandas.DataFrame(dis.index.to_series()).astype(numpy.int64)\n",
    "    int_steps.columns = ['timesteps']\n",
    "    time_steps = int_steps.to_records(index=False)\n",
    "    h5_file.create_table('/', 'time_steps', time_steps,\n",
    "                         'time steps for all catchments')\n",
    "    h5_file.close()\n",
    "\n",
    "\n",
    "def get_first_ids(q_file_name, max_ids):\n",
    "    \"\"\"Get first `max_ids` from the dicharge file.\n",
    "    \"\"\"\n",
    "    with open(q_file_name) as fobj:\n",
    "        header = next(fobj).strip().split(';')\n",
    "    return {int(entry[1:-1]) for entry in header[:max_ids]}\n",
    "\n",
    "\n",
    "def preprocess(config_file):\n",
    "    \"\"\"Do the preprocessing.\n",
    "    \"\"\"\n",
    "    config = read_config(config_file)\n",
    "    h5_file_name = config['preprocessing']['hdf_input_path']\n",
    "    t_file_name = config['preprocessing']['temperature_path']\n",
    "    p_file_name = config['preprocessing']['precipitation_path']\n",
    "    q_file_name = config['preprocessing']['discharge_path']\n",
    "    max_ids = config['preprocessing']['max_ids']\n",
    "    ids = None\n",
    "    if max_ids:\n",
    "        ids = get_first_ids(q_file_name, max_ids)\n",
    "    batch_size = config['preprocessing']['batch_size']\n",
    "    strahler = get_strahler(config['preprocessing']['catchment_path'], ids)\n",
    "    tot_areas = get_tot_areas(config['preprocessing']['catchment_path'], ids)\n",
    "    \n",
    "    crops = config['preprocessing']['crops'].split(', ')    \n",
    "    appl_areas = get_appl_areas(config['preprocessing']['landuse_path'], ids)\n",
    "\n",
    "    appl_rates = get_appl_rates(config['preprocessing']['micropollutant_path'], ids)\n",
    "    strahler_limit = config['preprocessing']['strahler_limit']\n",
    "    strahler, tot_areas, appl_areas, appl_rates = filter_strahler_lessthan(\n",
    "        strahler, tot_areas, appl_areas, appl_rates, strahler_limit)\n",
    "    create_hdf_file(h5_file_name, tot_areas, appl_areas, appl_rates)\n",
    "    add_input_tables(h5_file_name, t_file_name, p_file_name, q_file_name,\n",
    "                     batch_size=batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config = read_config('../config.ini')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h5_file_name = config['preprocessing']['hdf_input_path']\n",
    "t_file_name = config['preprocessing']['temperature_path']\n",
    "p_file_name = config['preprocessing']['precipitation_path']\n",
    "q_file_name = config['preprocessing']['discharge_path']\n",
    "max_ids = config['preprocessing']['max_ids']\n",
    "ids = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if max_ids:\n",
    "    ids = get_first_ids(q_file_name, max_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = config['preprocessing']['batch_size']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'e:\\\\moserand\\\\CrossWater\\\\4_Model\\\\4_2_Mass_Flow\\\\crosswater\\\\toy_model\\\\Daten\\\\GIS\\\\CATCHMENTS_Rhine.dbf'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-e3fc6ef4be4e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mstrahler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_strahler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'preprocessing'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'catchment_path'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-1-420691de5d2d>\u001b[0m in \u001b[0;36mget_strahler\u001b[1;34m(dbf_file_name, ids)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_strahler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdbf_file_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;34m\"\"\"Returns a dict with catchment ids as keys and strahler as values.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mget_value_by_id\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdbf_file_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'STRAHLER'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-420691de5d2d>\u001b[0m in \u001b[0;36mget_value_by_id\u001b[1;34m(dbf_file_name, col_name, converter, ids)\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[0mids\u001b[0m \u001b[0mto\u001b[0m \u001b[0mfilter\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0monly\u001b[0m \u001b[0mStrahler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m     \"\"\"\n\u001b[1;32m---> 48\u001b[1;33m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_dbf_cols\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdbf_file_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'WSO1_ID'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m     res = {id_: value * converter for id_, value in\n\u001b[0;32m     50\u001b[0m            zip(data['WSO1_ID'], data[col_name])}\n",
      "\u001b[1;32m<ipython-input-1-420691de5d2d>\u001b[0m in \u001b[0;36mread_dbf_cols\u001b[1;34m(dbf_file_name, col_names)\u001b[0m\n\u001b[0;32m     27\u001b[0m     \"\"\"\n\u001b[0;32m     28\u001b[0m     \u001b[0mdbf_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdbflib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDbfReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdbf_file_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m     \u001b[0mdbf_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcol_names\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdbf_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\moserand\\CrossWater\\4_Model\\4_2_Mass_Flow\\crosswater\\crosswater\\tools\\dbflib.py\u001b[0m in \u001b[0;36mread_all\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    167\u001b[0m         \"\"\"Read header and data.\n\u001b[0;32m    168\u001b[0m         \"\"\"\n\u001b[1;32m--> 169\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_header\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\moserand\\CrossWater\\4_Model\\4_2_Mass_Flow\\crosswater\\crosswater\\tools\\dbflib.py\u001b[0m in \u001b[0;36mread_header\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     97\u001b[0m         \"\"\"Read header information.\n\u001b[0;32m     98\u001b[0m         \"\"\"\n\u001b[1;32m---> 99\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m         self.numrec, lenheader = struct.unpack('<xxxxLH22x',\n\u001b[0;32m    101\u001b[0m                                                self.fobj.read(32))\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'e:\\\\moserand\\\\CrossWater\\\\4_Model\\\\4_2_Mass_Flow\\\\crosswater\\\\toy_model\\\\Daten\\\\GIS\\\\CATCHMENTS_Rhine.dbf'"
     ]
    }
   ],
   "source": [
    "strahler = get_strahler(config['preprocessing']['catchment_path'], ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dbf_file_name=config['preprocessing']['catchment_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e:\\\\moserand\\\\CrossWater\\\\4_Model\\\\4_2_Mass_Flow\\\\crosswater\\\\toy_model\\\\Daten\\\\GIS\\\\CATCHMENTS_Rhine.dbf'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbf_file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e:\\\\moserand\\\\CrossWater\\\\4_Model\\\\4_2_Mass_Flow\\\\crosswater\\\\toy_model\\\\Daten\\\\GIS\\\\CATCHMENTS_Rhine.dbf'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "dbf_file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'e:\\\\moserand\\\\CrossWater\\\\4_Model\\\\4_2_Mass_Flow\\\\crosswater\\\\toy_model\\\\Daten\\\\GIS'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-22a228879afd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'e:\\moserand\\CrossWater\\4_Model\\4_2_Mass_Flow\\crosswater\\toy_model\\Daten\\GIS'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'e:\\\\moserand\\\\CrossWater\\\\4_Model\\\\4_2_Mass_Flow\\\\crosswater\\\\toy_model\\\\Daten\\\\GIS'"
     ]
    }
   ],
   "source": [
    "os.chdir(r'e:\\moserand\\CrossWater\\4_Model\\4_2_Mass_Flow\\crosswater\\toy_model\\Daten\\GIS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
